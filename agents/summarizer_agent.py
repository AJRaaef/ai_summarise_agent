# agents/summarizer_agent.py
"""
Summarizer Agent: builds a prompt and asks the LLM for a human-friendly report.
"""

from core_tools.llm_connector import generate_text
import yaml
import os

PROMPT_PATH = os.path.join(os.path.dirname(__file__), "..", "prompts", "summarizer_prompts.yaml")

def _load_prompt_template():
    try:
        with open(PROMPT_PATH, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
            return data.get("summary_prompt") or data.get("example_template")
    except Exception:
        # fallback template
        return (
            "You are an expert airline data analyst. Using the profile, semantic mapping, KPIs and anomalies below, "
            "write a short executive summary with headings, one-sentence recommendations, and an action list.\n\n"
            "Profile:\n{profile}\n\nSemantic mapping:\n{semantic}\n\nKPIs:\n{kpis}\n\nAnomalies:\n{anomalies}\n\nWrite the summary now."
        )

class SummarizerAgent:
    def __init__(self):
        self.template = _load_prompt_template()

    def summarize(self, profile: str, semantic_map: dict, kpis: dict, anomalies: dict, sample=None) -> str:
        # Prepare text blocks
        semantic_text = []
        for col, m in semantic_map.get("mapping", {}).items():
            semantic_text.append(f"{col} -> {m['best_match']} (score {m['score']:.2f})")
        semantic_block = "\n".join(semantic_text[:50])

        # kpis into short table
        kpi_lines = []
        for col, v in list(kpis.items())[:20]:
            kpi_lines.append(f"{col}: mean={v['mean']}, std={v['std']}, min={v['min']}, max={v['max']}")
        kpi_block = "\n".join(kpi_lines) if kpi_lines else "No numeric KPIs."

        anomaly_block = f"Anomaly count: {anomalies.get('count', 0)}"

        prompt = self.template.format(
            profile=profile,
            semantic=semantic_block,
            kpis=kpi_block,
            anomalies=anomaly_block
        )

        # call LLM
        try:
            return generate_text(prompt)
        except Exception as e:
            # fallback minimal summary
            return f"Automated summary could not be generated by LLM due to: {e}\n\nProfile:\n{profile}\n\nKPIs:\n{kpi_block}\n\nAnomalies:\n{anomaly_block}"
